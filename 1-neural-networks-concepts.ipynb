{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks & Deep Learning\n",
    "### Marc Pomar - DataMad0120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Networks types and concept definitions\n",
    "Here are some basic network architectures you must know before starting. However in most cases architectures are combined to make the most of each network.\n",
    "\n",
    "* **Multilayer perceptron (MLPs):** Fully connected network, aka deep feedfoward networks. Those networks are common in simple logistic an linear regression problems. Not optimal decision for sequentials and multi-dimensional data patterns, require lots of parameters to barely work.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ledell/sldm4-h2o/master/mlp_network.png\" width=\"300\">\n",
    "\n",
    "* **Recurrent neural networks (RNNs):** Best for sequential data input (time series, audio classification, voice recognition, etc.). Those networks remember past states or have some kind of short term and long term past memory.\n",
    "\n",
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_6/recurrent.jpg\" width=\"300\">\n",
    "\n",
    "* **Convolutional Neural Networks (CNNs):** Best use case for image tasks or multidimensional data (images, videos, deep stereo images, etc.), those networks use the [convolution](https://en.wikipedia.org/wiki/Convolution) operation to learn feature kernels on data (specially images). Think of it as decomposing a big image classification problem in small parts, like recognizing a face can be decomposed in recognizing eye, mouth, nose, etc.\n",
    "\n",
    "<img src=\"https://i0.wp.com/vinodsblog.com/wp-content/uploads/2018/10/CNN-2.png?resize=1300%2C479&ssl=1\" width=\"300\">\n",
    "\n",
    "### 1.1 Concepts to know\n",
    "\n",
    "* **Loss function / Objective:** The goal when training a neural netwokr is to reduce the output value of the loss function. This is a powerful indicative that your network is learning, but remember to evaluate the network with test data to ensure quality.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"https://www.researchgate.net/profile/Victor_Suarez-Paniagua/publication/334643403/figure/fig3/AS:783985458302977@1563928107841/The-training-stage-of-a-Neural-Network-where-the-loss-function-is-decreasing-in-each.png\" width=\"230\" style=\"display:inline-block\">\n",
    "\n",
    "<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/11/Line-Plots-of-Sparse-Cross-Entropy-Loss-and-Classification-Accuracy-over-Training-Epochs-on-the-Blobs-Multi-Class-Classification-Problem.png\" width=\"300\" style=\"display:inline-block\">\n",
    "</div>\n",
    "\n",
    "\n",
    "* **Optimizer:** An iterative method for optimizing an objective/loss function. Basically, the algorithm that analyzes actual network neuron parameters and tune those to adjust for better output \n",
    "    - Stochastic Gradient Descent: https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "    - AdaGrad: http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\n",
    "    - Adam/Adamax: https://arxiv.org/abs/1412.6980v8\n",
    "    \n",
    "Optimizer algorithms visualized https://bl.ocks.org/EmilienDupont/aaf429be5705b219aaaf8d691e27ca87\n",
    "\n",
    "More optimizers implementations info here: \n",
    "```\n",
    "- https://keras.io/optimizers/\n",
    "- https://towardsdatascience.com/why-visualize-gradient-descent-optimization-algorithms-a393806eee2\n",
    "- https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/\n",
    "```\n",
    "\n",
    "    \n",
    "<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/05/Comparison-of-Adam-to-Other-Optimization-Algorithms-Training-a-Multilayer-Perceptron.png\" width=\"300\">\n",
    "\n",
    "\n",
    "* **Regularizer:** The role of the regularizer is to ensure that the trained model generalizes to new data by not overfitting the trained network. \n",
    "    - https://towardsdatascience.com/over-fitting-and-regularization-64d16100f45c\n",
    "\n",
    "<img src=\"https://www.bogotobogo.com/python/scikit-learn/images/NeuralNetwork7-Overfitting/Overfitting.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How does a network work\n",
    "\n",
    "This is a neuron:\n",
    "<img src=\"https://miro.medium.com/max/880/1*vGj29ZBD1kH1kDlGQspPxA.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
